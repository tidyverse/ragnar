<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>ragnar • ragnar</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.10/font.css" rel="stylesheet">
<link href="../deps/Source_Code_Pro-0.4.10/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="ragnar">
<meta name="robots" content="noindex">
<script src="https://cdn.jsdelivr.net/gh/posit-dev/supported-by-posit/js/badge.min.js" data-max-height="43" data-light-bg="#666f76" data-light-fg="#f9f9f9"></script><script defer data-domain="ragnar.tidyverse.org,all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-none" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">ragnar</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">0.2.1.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/ragnar.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tidyverse/ragnar/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article" id="container">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>ragnar</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/tidyverse/ragnar/blob/main/vignettes/ragnar.Rmd" class="external-link"><code>vignettes/ragnar.Rmd</code></a></small>
      <div class="d-none name"><code>ragnar.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ragnar.tidyverse.org/">ragnar</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="getting-started-with-ragnar">Getting Started with ragnar<a class="anchor" aria-label="anchor" href="#getting-started-with-ragnar"></a>
</h2>
<p>Retrieval-Augmented Generation (RAG) is a practical technique for
improving large language model (LLM) outputs by grounding them with
external, trusted content. The <code>ragnar</code> package provides
tools for building RAG workflows in R, with a focus on transparency and
control at each step.</p>
<p>This guide walks through building a simple chat tool for Quarto
documentation using <code>ragnar</code>. The code examples are
simplified for clarity; for a full implementation, see <a href="https://github.com/t-kalinowski/quartohelp" class="external-link uri">https://github.com/t-kalinowski/quartohelp</a>.</p>
<div class="section level3">
<h3 id="why-rag-the-hallucination-problem">Why RAG? The Hallucination Problem<a class="anchor" aria-label="anchor" href="#why-rag-the-hallucination-problem"></a>
</h3>
<p>LLMs can produce remarkable outputs: fluent, confident, plausible
responses to a wide range of prompts. But anyone who has spent time with
ChatGPT or similar models has observed responses that are confident,
plausible, and wrong.</p>
<p>When the generated output is wrong, we call that a
<em>hallucination</em>, and hallucinations seem to be an inherent
consequence of how LLMs work. LLMs operate on text sequences; they do
not seem to possess a concept of “facts” and “truth” like humans do.
They generate text with no awareness of whether it is true or false,
only guided by similarity to patterns in text sequences in their
training data.</p>
<p>Put simply, in philosopher Harry Frankfurt’s sense of the word, the
models generate “bullshit” <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;&lt;a href="https://press.princeton.edu/books/hardcover/9780691122946/on-bullshit" class="external-link uri"&gt;https://press.princeton.edu/books/hardcover/9780691122946/on-bullshit&lt;/a&gt;&lt;/p&gt;'><sup>1</sup></a>:</p>
<blockquote>
<p>It is impossible for someone to lie unless he thinks he knows the
truth. Producing bullshit requires no such conviction. A person who lies
is thereby responding to the truth, and he is to that extent respectful
of it. When an honest man speaks, he says only what he believes to be
true; and for the liar, it is correspondingly indispensable that he
considers his statements to be false. For the bullshitter, however, all
these bets are off: he is neither on the side of the true nor on the
side of the false. His eye is not on the facts at all, as the eyes of
the honest man and of the liar are, except insofar as they may be
pertinent to his interest in getting away with what he says. He does not
care whether the things he says describe reality correctly. He just
picks them out, or makes them up, to suit his purpose.</p>
</blockquote>
<p>RAG addresses this by retrieving relevant excerpts from a corpus of
trusted, vetted sources and asking the LLM to summarize, paraphrase, or
answer the user’s question using only that material. This grounds the
response in known content and reduces the risk of hallucination. RAG
shifts the LLM’s job from open-ended generation to summarizing or
quoting from retrieved material.</p>
<p>RAG reduces but does not eliminate hallucinations. For richer texts
and tasks, LLMs may still miss nuance or overgeneralize. For this
reason, it’s helpful if RAG-based tools present links back to the
original material so users can check context and verify details.</p>
</div>
<div class="section level3">
<h3 id="use-case-quarto-docs-chat-vs--standard-search">Use Case: Quarto Docs Chat vs. Standard Search<a class="anchor" aria-label="anchor" href="#use-case-quarto-docs-chat-vs--standard-search"></a>
</h3>
<p>Standard documentation search is the default for answering questions
about tools like Quarto but has limitations. Search requires precise
wording, familiarity with the docs’ structure, and sometimes piecing
together information from multiple pages. Even with a focused site
search, users spend time skimming and navigating to the correct
material, and may still miss the answer.</p>
<p>A RAG-powered chat tool could offer a better alternative. You ask a
natural language question. The tool retrieves relevant excerpts from the
docs using both semantic and keyword-based search, then asks the LLM to
answer using only those excerpts. The result is a concise, context-aware
answer, complete with links to the relevant docs. Of course, that only
is useful if the LLM actually provides correct and useful answers.</p>
</div>
<div class="section level3">
<h3 id="setting-up-rag">Setting up RAG<a class="anchor" aria-label="anchor" href="#setting-up-rag"></a>
</h3>
<p>At a high level, setting up RAG has two stages: preparing the
knowledge store (a database of processed content), and establishing the
workflow for retrieval and chat.</p>
<div class="section level4">
<h4 id="creating-the-store">Creating the Store<a class="anchor" aria-label="anchor" href="#creating-the-store"></a>
</h4>
<p>First, create a store. The store holds your processed docs and
embeddings. When you create the store, you select the embedding
provider. This choice is fixed for the store, but you can always create
a new store if you want to change it.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">store_location</span> <span class="op">&lt;-</span> <span class="st">"quarto.ragnar.duckdb"</span></span>
<span><span class="va">store</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ragnar_store_create.html">ragnar_store_create</a></span><span class="op">(</span></span>
<span>  <span class="va">store_location</span>,</span>
<span>  embed <span class="op">=</span> \<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">ragnar</span><span class="fu">::</span><span class="fu"><a href="../reference/embed_ollama.html">embed_openai</a></span><span class="op">(</span><span class="va">x</span>, model <span class="op">=</span> <span class="st">"text-embedding-3-small"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>To generate embeddings, you can use an open-source model via
<code><a href="../reference/embed_ollama.html">embed_ollama()</a></code>, models from commercial providers via
<code><a href="../reference/embed_ollama.html">embed_openai()</a></code>, <code><a href="../reference/embed_google_vertex.html">embed_google_vertex()</a></code>,
<code><a href="../reference/embed_bedrock.html">embed_bedrock()</a></code>, and <code><a href="../reference/embed_databricks.html">embed_databricks()</a></code>, or
your own function.</p>
</div>
<div class="section level4">
<h4 id="identify-documents-for-processing">Identify Documents for Processing<a class="anchor" aria-label="anchor" href="#identify-documents-for-processing"></a>
</h4>
<p>Gather a list of documents you want to insert in the database. For
local files, this can be a simple <code><a href="https://rdrr.io/r/base/list.files.html" class="external-link">list.files()</a></code> on a
directory of documents.</p>
<p>If you’re building a store from a website, you can use
<code><a href="../reference/ragnar_find_links.html">ragnar_find_links()</a></code> to collect URLs.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">paths</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ragnar_find_links.html">ragnar_find_links</a></span><span class="op">(</span><span class="st">"https://quarto.org/"</span>, depth <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<p>For some sites, it may be easier to clone and build the site locally,
then reference the files from the local file system. You can also
process a <a href="https://quarto.org/sitemap.xml" class="external-link">sitemap</a> if one is
available.</p>
<p>At the end of this step, you should have a character vector of file
paths and URLs.</p>
</div>
<div class="section level4">
<h4 id="convert-documents-to-markdown">Convert Documents to Markdown<a class="anchor" aria-label="anchor" href="#convert-documents-to-markdown"></a>
</h4>
<p>Convert each document to markdown. Markdown is preferred because it’s
plain text, easy to inspect, keeps token counts low, and works well for
both humans and LLMs.</p>
<p>For this step, ragnar provides <code><a href="../reference/read_as_markdown.html">read_as_markdown()</a></code>, which
can accept a wide variety of formats (pdf, docx, pptx, html, zip files,
epubs, etc.). In many cases it works well, but for specialized needs you
can opt for a more custom-tailored approach. See the help in
<code><a href="../reference/read_as_markdown.html">?read_as_markdown</a></code> for some guidance on alternatives if
you’d like to improve on the default conversion. (But only begin
optimizing once you have a basic app working.)</p>
<p><code><a href="../reference/read_as_markdown.html">read_as_markdown()</a></code> returns a
<code>MarkdownDocument</code> object, which is a normalized string of
markdown text with <code>@origin</code> property.</p>
<p>If you opt to use something besides <code><a href="../reference/read_as_markdown.html">read_as_markdown()</a></code>
to read in content–such as <code><a href="https://rdrr.io/r/base/readLines.html" class="external-link">readLines()</a></code>,
<code>pdftools::pdf_text()</code>, or a sophisticated OCR tool–you can
turn a character vector into a <code><a href="../reference/MarkdownDocument.html">ragnar::MarkdownDocument</a></code>
object with the <code><a href="../reference/MarkdownDocument.html">MarkdownDocument()</a></code> constructor.</p>
</div>
<div class="section level4">
<h4 id="chunk-and-augment">Chunk and Augment<a class="anchor" aria-label="anchor" href="#chunk-and-augment"></a>
</h4>
<p>Next, split the documents into smaller chunks. This is necessary
because embedding models have context size limits, and because chunking
allows you to return just the most relevant excerpts from a long
document.</p>
<p>Chunking is delicate; Ideally, each chunk should stand alone without
relying on the context of the surrounding document. We can aim to split
the text at natural points like headings or paragraphs, and avoid splits
in the middle of a sentence or word.</p>
<p>Additionally, we can augment chunks with context that describes the
chunk’s origin–such as URL, title, headings, and subheadings–both so the
LLM can provide links back to the source, and so the LLM and embedding
models can better situate the chunk’s content.</p>
<p>To help with these tasks, use <code><a href="../reference/markdown_chunk.html">markdown_chunk()</a></code>.</p>
<p><code><a href="../reference/markdown_chunk.html">markdown_chunk()</a></code> splits the document into chunks and
nudges each chunk’s edges to the nearest semantic break. By default, a
chunk is about 1,600 characters–roughly one page–with a 50% overlap
between chunks.</p>
<p><code>markdown_chunk</code> also extracts any markdown headings in
scope for each chunk start. These headings are added as context during
embedding and retrieval, helping the LLM produce better answers.</p>
<p>You can specify chunking boundaries with
<code><a href="../reference/markdown_chunk.html">markdown_chunk()</a></code>’s <code>segment_by_heading_levels</code>
argument, which takes a vector of integers between 1 and 6. Chunks will
not overlap a defined segment boundary.</p>
<p>Note that an alternative approach for augmenting chunks with context
can be to use an LLM with instructions to “situate this excerpt from
this document,” or, worse, “summarize this document.” This can work but
carries significant risk. Remember, the goal is to create a knowledge
store–a trusted, factual, vetted source of truth. Giving an LLM an
opportunity to corrupt this store with hallucinations may be necessary
depending on your needs, but as an initial approximation, I recommend
starting with an ingestion pipeline that does not give any opportunities
for hallucinations to enter the store.</p>
</div>
<div class="section level4">
<h4 id="insert-in-the-store">Insert in the Store<a class="anchor" aria-label="anchor" href="#insert-in-the-store"></a>
</h4>
<p>Take your augmented document chunks and insert them into the store by
calling <code><a href="../reference/ragnar_store_insert.html">ragnar_store_insert()</a></code>. This function will
automatically generate embeddings using the <code>embed</code> function
specified when the store was first created.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/ragnar_store_insert.html">ragnar_store_insert</a></span><span class="op">(</span><span class="va">store</span>, <span class="va">chunks</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="tying-it-together">Tying it Together<a class="anchor" aria-label="anchor" href="#tying-it-together"></a>
</h4>
<p>Repeat these steps for every document you want to insert into the
store. Once you’re done processing the documents, call
<code><a href="../reference/ragnar_store_build_index.html">ragnar_store_build_index()</a></code> to finalize the store and build
the index.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">path</span> <span class="kw">in</span> <span class="va">paths</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">chunks</span> <span class="op">&lt;-</span> <span class="va">path</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/read_as_markdown.html">read_as_markdown</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/markdown_chunk.html">markdown_chunk</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="../reference/ragnar_store_insert.html">ragnar_store_insert</a></span><span class="op">(</span><span class="va">store</span>, <span class="va">chunks</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="../reference/ragnar_store_build_index.html">ragnar_store_build_index</a></span><span class="op">(</span><span class="va">store</span><span class="op">)</span></span></code></pre></div>
<p>Once the store index is built, the store is ready for retrieval.</p>
<hr>
</div>
</div>
<div class="section level3">
<h3 id="retrieval">Retrieval<a class="anchor" aria-label="anchor" href="#retrieval"></a>
</h3>
<p>To retrieve content from the store, call
<code><a href="../reference/ragnar_retrieve.html">ragnar_retrieve()</a></code>. This function uses two retrieval
methods:</p>
<ul>
<li>
<strong>Vector similarity search (vss):</strong> Retrieves chunks
whose embeddings are most similar to the query embedding. This is
semantic search, used to find content conceptually related to the query,
even if different words are used.</li>
<li>
<strong>BM25:</strong> Retrieves chunks based on keyword matching,
using techniques like stemming and term frequency. This is a
conventional text search, used to find content containing specific words
or phrases.</li>
</ul>
<p>To limit the search to one method, use
<code><a href="../reference/ragnar_retrieve_vss.html">ragnar_retrieve_vss()</a></code> or
<code><a href="../reference/ragnar_retrieve_bm25.html">ragnar_retrieve_bm25()</a></code>.</p>
<p>You can register <code><a href="../reference/ragnar_retrieve.html">ragnar_retrieve()</a></code> as an LLM tool. This
is an effective technique for implementing RAG, as it allows the LLM to
rephrase unclear questions, ask follow-up questions, or search more than
once if needed. Register <code><a href="../reference/ragnar_retrieve.html">ragnar_retrieve()</a></code> as a tool with
<code><a href="https://ellmer.tidyverse.org/reference/Chat.html" class="external-link">ellmer::Chat</a></code> using
<code><a href="../reference/ragnar_register_tool_retrieve.html">ragnar_register_tool_retrieve()</a></code>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">client</span> <span class="op">&lt;-</span> <span class="fu">ellmer</span><span class="fu">::</span><span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_openai.html" class="external-link">chat_openai</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/ragnar_register_tool_retrieve.html">ragnar_register_tool_retrieve</a></span><span class="op">(</span></span>
<span>  <span class="va">client</span>, <span class="va">store</span>, top_k <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  description <span class="op">=</span> <span class="st">"the quarto website"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Note that the registered tool is intentionally simple. It asks the
LLM to provide one argument: the query string. LLM tool calls are just
text completions after all, like any other LLM output. We minimize the
complexity in the tool interface to minimize opportunities for LLM to
make errors.</p>
<p>Rather than exposing detailed search options to the LLM, we can
instead set a high <code>top_k</code> value to return more chunks than
usually necessary. This provides some slack in the chat app, so we can
gracefully handle less-than-perfectly-ranked search results.</p>
<div class="section level4">
<h4 id="customizing-retrieval">Customizing Retrieval<a class="anchor" aria-label="anchor" href="#customizing-retrieval"></a>
</h4>
<p>For more context-specific tasks, you may want to define your own
retrieval tool and pair it with a system prompt that explains how to use
the results.</p>
<p>For example, suppose you want the LLM to perform repeated searches if
the first search does not return relevant information, and you also want
to ensure repeated searches do not return previously seen chunks. Here’s
an example of how you might do this</p>
<p>First, set up the system prompt:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">client</span> <span class="op">&lt;-</span> <span class="fu">chat_openai</span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4.1"</span><span class="op">)</span></span>
<span><span class="va">client</span><span class="op">$</span><span class="fu">set_system_prompt</span><span class="op">(</span><span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/trim.html" class="external-link">trim</a></span><span class="op">(</span></span>
<span>  <span class="st">"</span></span>
<span><span class="st">  You are an expert in Quarto documentation. You are concise.</span></span>
<span><span class="st">  Always perform a search of the Quarto knowledge store for each user request.</span></span>
<span><span class="st">  If the initial search does not return relevant documents, you may perform</span></span>
<span><span class="st">  up to three additional searches. Each search will return unique, new excerpts.</span></span>
<span><span class="st">  If no relevant results are found, inform the user and do not attempt to answer the question.</span></span>
<span><span class="st">  If the user request is ambiguous, perform at least one search first, then ask a clarifying question.</span></span>
<span><span class="st"></span></span>
<span><span class="st">  Every response must cite links to official documentation sources.</span></span>
<span><span class="st">  Always include a minimal, fully self-contained Quarto document in your answer.</span></span>
<span><span class="st">  "</span></span>
<span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Next, define a custom tool:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rag_retrieve_quarto_excerpts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eval.html" class="external-link">local</a></span><span class="op">(</span><span class="op">{</span></span>
<span>  <span class="va">retrieved_chunk_ids</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">integer</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="kw">function</span><span class="op">(</span><span class="va">text</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co"># Search, excluding previously seen chunks</span></span>
<span>    <span class="va">chunks</span> <span class="op">&lt;-</span> <span class="fu">ragnar</span><span class="fu">::</span><span class="fu"><a href="../reference/ragnar_retrieve.html">ragnar_retrieve</a></span><span class="op">(</span></span>
<span>      <span class="va">text</span>,</span>
<span>      top_k <span class="op">=</span> <span class="fl">10</span>,</span>
<span>      filter <span class="op">=</span> <span class="op">!</span><span class="va">.data</span><span class="op">$</span><span class="va">chunk_id</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="va">retrieved_chunk_ids</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Update seen chunks</span></span>
<span>    <span class="va">retrieved_chunk_ids</span> <span class="op">&lt;&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unique.html" class="external-link">unique</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/unlist.html" class="external-link">unlist</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">retrieved_chunk_ids</span>, <span class="va">chunks</span><span class="op">$</span><span class="va">chunk_id</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Return the chunks dataframe directly;</span></span>
<span>    <span class="co"># ellmer will format this to json as a row-oriented list of objects.</span></span>
<span>    <span class="va">chunks</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<p>This approach presents retrieved content directly in json. Presenting
content in a defined structure helps prevent the LLM from confusing
retrieved content with user queries, and confusing chunk metadata (like
headings or origin) with the chunk text. The row-oriented format also
ensures that all metadata (such as headings) stays attached to each row,
so the context for each chunk appears next to its content.</p>
<p>Register the custom tool:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">client</span><span class="op">$</span><span class="fu">register_tool</span><span class="op">(</span><span class="fu">ellmer</span><span class="fu">::</span><span class="fu"><a href="https://ellmer.tidyverse.org/reference/tool.html" class="external-link">tool</a></span><span class="op">(</span></span>
<span>  <span class="va">rag_retrieve_quarto_excerpts</span>,</span>
<span>  <span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/trim.html" class="external-link">trim</a></span><span class="op">(</span></span>
<span>    <span class="st">"</span></span>
<span><span class="st">    Use this tool to retrieve the most relevant excerpts from the Quarto</span></span>
<span><span class="st">    knowledge store for a given text input. This function:</span></span>
<span><span class="st">    - uses both vector (semantic) similarity and BM25 text search,</span></span>
<span><span class="st">    - never returns the same excerpt twice in the same session</span></span>
<span><span class="st">    "</span></span>
<span>  <span class="op">)</span>,</span>
<span>  text <span class="op">=</span> <span class="fu">ellmer</span><span class="fu">::</span><span class="fu"><a href="https://ellmer.tidyverse.org/reference/type_boolean.html" class="external-link">type_string</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="troubleshooting-and-debugging">Troubleshooting and Debugging<a class="anchor" aria-label="anchor" href="#troubleshooting-and-debugging"></a>
</h3>
<p>Developing a RAG app is an iterative process. There are many places
to potentially spend effort on improvements:</p>
<ul>
<li>selecting sources</li>
<li>converting to markdown</li>
<li>chunking</li>
<li>augmenting chunks</li>
<li>using metadata to narrow search</li>
<li>choice of embedding model</li>
<li>choice of LLM</li>
<li>system prompt</li>
<li>tool definition</li>
</ul>
<p>It’s helpful to iterate in the context of an end-to-end
application.</p>
<p>You can use <code><a href="../reference/ragnar_store_inspect.html">ragnar_store_inspect()</a></code> to interactively see
what kinds of results are returned by the store for different queries.
This helps confirm that chunking and augmentation preserve semantic
meaning and that the embedding model is working as expected.</p>
<p>If the results shown in the inspector do not seem useful or relevant
to you, they likely won’t be useful to an LLM either. Iterate on the
store creation pipeline until retrieval returns meaningful excerpts.</p>
<p><img src="../reference/figures/ragnar-store-inspector-screenshot.png"></p>
<p>Some things you can try:</p>
<ul>
<li>Increase the chunk size.</li>
<li>Specify coarser or custom boundaries in
<code><a href="../reference/markdown_chunk.html">markdown_chunk()</a></code>.</li>
<li>Augment chunks with additional context.</li>
<li>Try a different embedding model.</li>
<li>Try a different LLM.</li>
<li>Increase <code>top_k</code> to return more results.</li>
<li>Iterate on the LLM system prompt to give clearer, more precise
instructions.</li>
</ul>
<p>To access the store data directly, use
<code>tbl(store@con, "chunks")</code>. With store version 2, you can
also access <code>tbl(store@con, "documents")</code>. You can use dbplyr
verbs to operate on the remote tbl, and convert it to an in-memory
tibble with <code><a href="https://dplyr.tidyverse.org/reference/compute.html" class="external-link">dplyr::collect()</a></code>.</p>
<p>Chat interfaces and LLM marketing invite us to think of LLMs as
general-purpose agents, able to answer anything. In practice, however,
as of 2025, building a reliable, accurate, LLM-powered solution where
details and facts matter means carefully scoping what the model is
responsible for.</p>
<p>With that in mind, note that this chat app described here does not
intend to replace documentation or act as a general-purpose assistant.
Its goal is to provide a faster, more contextual way to find the right
place in the docs, with enough information for the user to decide if
they need to read further. It’s designed it to allow users to naturally
escalate: if the LLM is not able to provide a useful answer, the user
can use the provided links and transition to reading the source material
without friction.</p>
</div>
<div class="section level3">
<h3 id="cost-management">Cost Management<a class="anchor" aria-label="anchor" href="#cost-management"></a>
</h3>
<p>Using LLMs and embeddings incurs costs, regardless of whether you use
a commercial provider or an open source model on your own hardware. Some
tips for managing costs:</p>
<ul>
<li>Use a model with a large context window so you can include more
context, but not necessarily the most expensive reasoning model. With
RAG, summarization and paraphrasing do not typically need the flagship
reasoning models to return useful results.</li>
<li>The cost of generating embeddings is negligible. For example,
generating embeddings for the entire Quarto website costs approximately
a cent.</li>
<li>Keep chat sessions focused and concise. Start new chat sessions for
unrelated questions. Long conversations increase token usage, costs, and
in practice, also lower output quality as the LLM gets confused by stale
or irrelevant conversation turns.</li>
<li>For reference, with the flagship OpenAI model ‘gpt-4.1’, each query
to a RAG-powered chat app like <code>quartohelp</code> costs
approximately 1 cent.</li>
</ul>
</div>
<div class="section level3">
<h3 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h3>
<p><code>ragnar</code> provides a practical, transparent way to build
RAG workflows in R. By combining semantic and keyword search, clear
chunking and augmentation, and focused prompt and tool design, you can
create fast, interactive documentation chat tools that help users find
answers quickly and reliably.</p>
<p>Building a good RAG system is iterative. Inspect intermediate
outputs, tune chunking and retrieval, and keep the user’s workflow in
mind. With these guardrails, you can reduce hallucinations and deliver
trustworthy, grounded answers–while also giving users a path to the
original source.</p>
<p>For more details and a full example, see the <a href="https://github.com/t-kalinowski/quartohelp" class="external-link">quartohelp</a>
package.</p>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p>Developed by Tomasz Kalinowski, Daniel Falbel, <a href="https://www.posit.co" class="external-link"><img src="https://www.tidyverse.org/posit-logo.svg" alt="Posit" height="16" width="62" style="margin-bottom: 3px;"></a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

  </div></footer>
</body>
</html>
